{"name":"Pml proj","tagline":"","body":"---\r\ntitle: 'Coursera: Practical Machine Learning Project'\r\nauthor: \"palm1221ks\"\r\ndate: \"Sunday, July 26, 2015\"\r\noutput: html_document\r\n---\r\n\r\n##Executive Summary\r\nThis analysis seeks to predict which of 5 weightlifting activities are being performed based on data collected by sensors placed on participants. The data was collected in a highly controlled environment by trained professionals to ensure participants performed the activities with proper form and consistency. Overall, due to the efforts to maintain strict control over the movements, it is possible to predict the movements with high accuracy.\r\n\r\n###Loading Data\r\n\r\nTraining Data is downloaded from: \r\n<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv> \r\n\r\nTesting Data is downloaded from: \r\n<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv> \r\n\r\n###Data Cleaning and Formatting\r\nThe data sets provided include a number of data points with NA or blank values, therefore it is necessary to clean the data set and remove them. I chose to remove the columns because the missing data represented a majority of the points from each column. Furthermore, the first 7 columns of the data are either not useful (participants name, time stamps) or bias the model (window number).\r\n\r\n```\r\nclean_na_cols <- function(x, p = .9) {\r\n  \r\n  #Create Blank Vector to record the NA Columns\r\n    k <- as.numeric(vector())\r\n  #Loop through each column and evaluate the number of NA's/Blanks\r\n    for(i in 1:length(x)){\r\n        \r\n        na_count <- sum(is.na(x[,i]))\r\n        blnk_cnt <- sum(x[,i]==\"\")\r\n        \r\n        col_len <- length(x[,i])\r\n        \r\n        na_prop <- na_count/col_len\r\n        bl_prop <- blnk_cnt/col_len\r\n        \r\n        if(na_prop<p & bl_prop<p){\r\n          k <- rbind(k,i)\r\n        }\r\n        \r\n    }\r\n  #Return the clean dataset\r\n    clean_na_cols <- x[,k]\r\n  \r\n}\r\n```\r\n\r\n###Model Selection\r\nUsing the Caret package I chose to run a random forest model with 30% for training and 70% for validation. I chose the small training sample simply due to processing performance. Furthermore, I pre-processed the data minimally by simply centering and scaling each feature.\r\n\r\n```\r\n#Remove the columns with mostly NAs\r\n    clean_train <- clean_na_cols(train_data)\r\n    clean_test <- clean_na_cols(test_data)\r\n  #Remove columns first 7 columns\r\n    clean_train <- clean_train[,-(1:7)]\r\n    clean_test <- clean_test[,-(1:7)]\r\n\r\ninTrain <- createDataPartition(clean_train$classe, p = .3, list = F)\r\n\r\ntraining <- clean_train[inTrain,]\r\ntesting <- clean_train[-inTrain,]\r\n\r\n#RandomForest Model\r\n  model <- train(classe ~ ., data = training, preProcess = c(\"center\",\"scale\"), method = \"rf\" )\r\n```\r\n\r\n###Cross-Validation\r\nThe data was cross-validated on 70% of the training data and achieved 98% accuracy which based on the size of the validation set suggests a very reliable estimate of the overall accuracy of this model.\r\n\r\n```\r\n#Cross Validation\r\n  validation <- predict(model, newdata = testing)\r\n  confusionMatrix(testing$classe, validation)\r\n```\r\n\r\n```\r\nConfusion Matrix and Statistics\r\n\r\n          Reference\r\nPrediction    A    B    C    D    E\r\n         A 3885    9    8    2    2\r\n         B   54 2579   21    1    2\r\n         C    2   55 2329    9    0\r\n         D    1    2   54 2188    6\r\n         E    1   16    7   18 2482\r\n\r\nOverall Statistics\r\n                                          \r\n               Accuracy : 0.9803          \r\n                 95% CI : (0.9779, 0.9826)\r\n    No Information Rate : 0.2871          \r\n    P-Value [Acc > NIR] : < 2.2e-16       \r\n                                          \r\n                  Kappa : 0.9751          \r\n Mcnemar's Test P-Value : < 2.2e-16       \r\n\r\nStatistics by Class:\r\n\r\n                     Class: A Class: B Class: C Class: D Class: E\r\nSensitivity            0.9853   0.9692   0.9628   0.9865   0.9960\r\nSpecificity            0.9979   0.9930   0.9942   0.9945   0.9963\r\nPos Pred Value         0.9946   0.9706   0.9724   0.9720   0.9834\r\nNeg Pred Value         0.9941   0.9926   0.9921   0.9974   0.9991\r\nPrevalence             0.2871   0.1938   0.1761   0.1615   0.1815\r\nDetection Rate         0.2829   0.1878   0.1696   0.1593   0.1807\r\nDetection Prevalence   0.2844   0.1935   0.1744   0.1639   0.1838\r\nBalanced Accuracy      0.9916   0.9811   0.9785   0.9905   0.9961\r\n```\r\n\r\n###Conclusion\r\n\r\nWith minimal effort this it was possible to predict the the correct activity with high accuracy. I imagine that with additional effort it would be possible to produce even better results approaching perfect accuracy. The high control maintained during the collection of the data make this possible and an effective learning exercise.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}